{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7X/GXCUosy1XODhaBSQLa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SouvikBandyopadhyay/CUDA_min_max_sum_matrixmul/blob/main/cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to Initialize CUDA development environment on Colab Notebook"
      ],
      "metadata": {
        "id": "2XYkmoKfrMgT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln9Eom1nYbo9",
        "outputId": "90b75f1d-e0fd-4165-c3a2-ba1a06efa70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin    cuda\tcuda-12.2  games\t       include\tlib64\t   man\t share\n",
            "colab  cuda-12\tetc\t   _gcs_config_ops.so  lib\tlicensing  sbin  src\n"
          ]
        }
      ],
      "source": [
        "!ls /usr/local/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!which nvcc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvLgecDeYq5u",
        "outputId": "61b4561f-cacd-4ba0-cede-18651987ca0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/nvcc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBrkFTiZYzYV",
        "outputId": "08e69d36-f336-4700-d642-0e451ea33ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec 16 07:49:09 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install nvidia-cuda-toolkit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0Nbq1C8dwEL",
        "outputId": "85fcf4da-42e0-4578-ba94-19a09835e0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "nvidia-cuda-toolkit is already the newest version (11.5.1-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This code finds the minimum and maximum value from an array using CUDA.\n",
        "# Kernel `min_max_kernel`:\n",
        "**1. Thread Index Calculation:**\n",
        "* `int tid = threadIdx.x + blockIdx.x * blockDim.x;` calculates the thread ID within the grid.\n",
        "\n",
        "**2. Local Min and Max Initialization:**\n",
        "* `int local_min = arr[0];` and `int local_max = arr[0];` initialize local variables with the first element of the array.\n",
        "\n",
        "**3. Find Local Min and Max:**\n",
        "* The kernel iterates through the array elements assigned to each thread.\n",
        "* Each thread checks a specific range of elements (`i`) within the array (`arr`) using the thread ID and grid dimensions.\n",
        "* For each element, it updates `local_min` and `local_max` if a smaller or larger value is found, respectively.\n",
        "\n",
        "**4. Atomic Operations:**\n",
        "* `atomicMin(min_val, local_min);` and `atomicMax(max_val, local_max);` atomically update the global minimum and maximum values (`min_val` and `max_val`) with the local minimum and maximum found by each thread.\n",
        "\n",
        "# `main()` Function:\n",
        "**1. Array Initialization and Allocation:**\n",
        "* Allocates memory for the array `arr`.\n",
        "* Initializes the array with random values and prints its elements.\n",
        "\n",
        "**2. Device Memory Allocation and Copying:**\n",
        "* Allocates memory on the GPU for `d_arr` (array data), `d_min`, and `d_max`.\n",
        "* Copies the array data from the host to the device.\n",
        "\n",
        "**3. Initialize Min and Max Values:**\n",
        "* Initializes `min_val` and `max_val` variables with the first element of the array.\n",
        "* Copies these initial values to device memory (`d_min` and `d_max`).\n",
        "\n",
        "**4. Kernel Launch:**\n",
        "* Calculates the number of blocks needed based on the array size and launches the `min_max_kernel` with the configured block size.\n",
        "* Each block processes a set of array elements to find local minimum and maximum values.\n",
        "\n",
        "**5. Copy Results Back to Host:**\n",
        "* Copies the updated minimum and maximum values from the device to the host.\n",
        "\n",
        "**6. Display Minimum and Maximum Values:**\n",
        "* Prints the minimum and maximum values found in the array.\n",
        "\n",
        "**7. Memory Cleanup:**\n",
        "* Frees the allocated memory on both the host and the device.\n",
        "\n",
        "\n",
        "\n",
        "In summary, this CUDA code employs parallel computation to swiftly identify the minimum and maximum values within an array using GPU-based kernels. By allocating threads to calculate local minimum and maximum values across the array segments and merging these values using atomic operations, the code efficiently determines the overall minimum and maximum. This implementation optimizes the computation by leveraging GPU parallelism, enhancing the speed of finding the array's extremes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hLZFERRwrGvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile minmax.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "__global__ void min_max_kernel(int* arr, int n, int* min_val, int* max_val) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int local_min = arr[0];\n",
        "    int local_max = arr[0];\n",
        "\n",
        "    for (int i = tid; i < n; i += blockDim.x * gridDim.x) {\n",
        "        if (arr[i] < local_min) {\n",
        "            local_min = arr[i];\n",
        "        }\n",
        "        if (arr[i] > local_max) {\n",
        "            local_max = arr[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    atomicMin(min_val, local_min);\n",
        "    atomicMax(max_val, local_max);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 10; // array size\n",
        "    int *arr, *d_arr, *d_min, *d_max;\n",
        "    int min_val, max_val;\n",
        "\n",
        "    // Allocate memory for the array\n",
        "    arr = (int*)malloc(n * sizeof(int));\n",
        "\n",
        "    // Initialize array with random values\n",
        "    printf(\"Inital Array\\n\");\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        arr[i] = rand() % 1000;\n",
        "        printf(\"elem%d = %d, \",i,arr[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    // Allocate memory on device\n",
        "    cudaMalloc((void**)&d_arr, n * sizeof(int));\n",
        "    cudaMalloc((void**)&d_min, sizeof(int));\n",
        "    cudaMalloc((void**)&d_max, sizeof(int));\n",
        "\n",
        "    // Copy array to device\n",
        "    cudaMemcpy(d_arr, arr, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Initialize min and max values\n",
        "    min_val = arr[0];\n",
        "    max_val = arr[0];\n",
        "    cudaMemcpy(d_min, &min_val, sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_max, &max_val, sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel\n",
        "    int num_blocks = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "    min_max_kernel<<<num_blocks, BLOCK_SIZE>>>(d_arr, n, d_min, d_max);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(&min_val, d_min, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(&max_val, d_max, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Minimum value: %d\\n\", min_val);\n",
        "    printf(\"Maximum value: %d\\n\", max_val);\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_arr);\n",
        "    cudaFree(d_min);\n",
        "    cudaFree(d_max);\n",
        "\n",
        "    // Free host memory\n",
        "    free(arr);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zIO9mAEY8d_",
        "outputId": "0f47a434-4c3a-42a7-a19d-9dd645d02daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting minmax.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc minmax.cu -o minmax"
      ],
      "metadata": {
        "id": "26r6iHJjb9os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./minmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkmaC-6UZcND",
        "outputId": "db05ee3e-a619-44f6-97ad-7a6f70750ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inital Array\n",
            "elem0 = 383, elem1 = 886, elem2 = 777, elem3 = 915, elem4 = 793, elem5 = 335, elem6 = 386, elem7 = 492, elem8 = 649, elem9 = 421, \n",
            "Minimum value: 335\n",
            "Maximum value: 915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This code calculates the summation of all elements in an array using CUDA.\n",
        "# Kernel `sum_reduce_kernel`:\n",
        "**1. Shared Memory Allocation:**\n",
        "* `extern __shared__ int shared[];` allocates shared memory on the GPU.\n",
        "\n",
        "**2. Thread Index Calculation:**\n",
        "* `int tid = threadIdx.x;` obtains the thread ID within the block.\n",
        "* `int idx = blockIdx.x * blockDim.x + threadIdx.x;` calculates the global index of the thread.\n",
        "\n",
        "**3. Loading Data into Shared Memory:**\n",
        "* `if (idx < n)` checks if the thread index is within the array bounds.\n",
        "* Each thread loads an element of the array into shared memory (`shared[tid] = arr[idx];`). If the thread is out of bounds, it loads 0.\n",
        "\n",
        "**4. Synchronization:**\n",
        "* `__syncthreads();` ensures all threads have loaded their data into shared memory before proceeding.\n",
        "\n",
        "**5. Parallel Reduction:**\n",
        "* The kernel performs a parallel reduction to find the sum of the array elements using shared memory.\n",
        "* It iteratively adds elements by reducing the stride in half until reaching 0.\n",
        "* Threads with `tid < stride` update their values by adding the value at `tid + stride` to their own value.\n",
        "* `__syncthreads();` ensures synchronization after each iteration.\n",
        "\n",
        "**6. Storing Partial Results:**\n",
        "* Once the reduction completes (`tid == 0`), each block stores its partial sum (`shared[0]`) into the `result` array at the corresponding block index.\n",
        "\n",
        "# `main()` Function:\n",
        "**1. Array Initialization and Allocation:**\n",
        "* Allocates memory for the array `arr`.\n",
        "* Initializes the array with random values and prints its elements.\n",
        "\n",
        "**2. Device Memory Allocation and Copying:**\n",
        "* Allocates memory on the GPU (`d_arr` for the array data, `d_result` for the final result).\n",
        "* Copies the array data from the host to the device.\n",
        "\n",
        "**3. Kernel Configuration:**\n",
        "* Calculates the grid and block sizes based on the array size (`n`) and the defined `BLOCK_SIZE`.\n",
        "\n",
        "**4. Temporary Memory Allocation:**\n",
        "* Allocates temporary memory (`temp_result`) on the GPU to store partial results.\n",
        "\n",
        "**5. Kernel Launch:**\n",
        "* Launches the kernel (`sum_reduce_kernel`) with the configured grid and block sizes to perform parallel reduction on the GPU.\n",
        "\n",
        "**6. Copying Partial Results to CPU:**\n",
        "* Copies the partial results from the device to the host.\n",
        "\n",
        "**7. Final Reduction on CPU:**\n",
        "* Performs the final reduction on the CPU by summing up the partial results obtained from each block.\n",
        "\n",
        "**8. Printing the Sum:**\n",
        "* Displays the sum of the array elements computed on the CPU.\n",
        "\n",
        "**9. Memory Cleanup:**\n",
        "* Frees the allocated memory on both the host and the device.\n",
        "\n",
        "\n",
        "In summary, this code utilizes CUDA to perform parallel reduction, efficiently summing the elements of an array. It uses GPU kernels to distribute workload across threads and blocks, optimizing computation via parallelism. The main() function initializes the array, transfers data to the GPU, launches the reduction kernel, retrieves and calculates the final sum on the CPU, showcasing CUDA's capacity for accelerating array summation tasks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JCWC4nU8qsSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sum.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "__global__ void sum_reduce_kernel(int* arr, int n, int* result) {\n",
        "    extern __shared__ int shared[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < n) {\n",
        "        shared[tid] = arr[idx];\n",
        "    } else {\n",
        "        shared[tid] = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            shared[tid] += shared[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        result[blockIdx.x] = shared[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 10; // array size\n",
        "    int *arr, *d_arr, *d_result;\n",
        "    int result_cpu = 0;\n",
        "\n",
        "    // Allocate memory for the array\n",
        "    arr = (int*)malloc(n * sizeof(int));\n",
        "\n",
        "    // Initialize array with random values\n",
        "    printf(\"Inital Array\\n\");\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        arr[i] = rand() % 1000;\n",
        "        printf(\"elem%d = %d, \",i,arr[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    // Allocate memory on device\n",
        "    cudaMalloc((void**)&d_arr, n * sizeof(int));\n",
        "    cudaMalloc((void**)&d_result, sizeof(int));\n",
        "\n",
        "    // Copy array to device\n",
        "    cudaMemcpy(d_arr, arr, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Calculate grid and block sizes\n",
        "    int block_size = BLOCK_SIZE;\n",
        "    int grid_size = (n + block_size - 1) / block_size;\n",
        "\n",
        "    // Allocate temporary storage for partial sums\n",
        "    int* temp_result;\n",
        "    cudaMalloc((void**)&temp_result, grid_size * sizeof(int));\n",
        "\n",
        "    // Launch kernel for sum reduction\n",
        "    sum_reduce_kernel<<<grid_size, block_size, block_size * sizeof(int)>>>(d_arr, n, temp_result);\n",
        "\n",
        "    // Reduce the partial sums on the CPU\n",
        "    int* partial_result = (int*)malloc(grid_size * sizeof(int));\n",
        "    cudaMemcpy(partial_result, temp_result, grid_size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    for (int i = 0; i < grid_size; ++i) {\n",
        "        result_cpu += partial_result[i];\n",
        "    }\n",
        "\n",
        "    printf(\"Sum of array elements : %d\\n\", result_cpu);\n",
        "\n",
        "    // Free memory\n",
        "    cudaFree(d_arr);\n",
        "    cudaFree(d_result);\n",
        "    cudaFree(temp_result);\n",
        "    free(arr);\n",
        "    free(partial_result);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbvE07KPkS7p",
        "outputId": "5903de30-d18d-407d-c0a0-b2d8acdd8999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sum.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc sum.cu -o sum"
      ],
      "metadata": {
        "id": "Ou-_X4ATkniJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kECdHBmXkqPv",
        "outputId": "bc1b2e7b-16af-4dd8-ac0f-5635934c59f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inital Array\n",
            "elem0 = 383, elem1 = 886, elem2 = 777, elem3 = 915, elem4 = 793, elem5 = 335, elem6 = 386, elem7 = 492, elem8 = 649, elem9 = 421, \n",
            "Sum of array elements : 6037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This code performs matrix multiplication of two 4x4 matrices (A and B) using CUDA.\n",
        "\n",
        "**1. Matrix Initialization:**\n",
        "* Two 4x4 matrices, A and B, are initialized in the main() function.\n",
        "\n",
        "**2. Memory Allocation and Copying to Device:**\n",
        "* Memory is allocated on the GPU for matrices d_A, d_B, and d_C using cudaMalloc.\n",
        "* The contents of matrices A and B are copied from the host (CPU) to the device (GPU) using cudaMemcpy.\n",
        "\n",
        "**3. Kernel Definition (matrix_multiply):**\n",
        "* The matrix_multiply kernel is launched on the GPU to perform the matrix multiplication.\n",
        "* Each thread calculates a single element of the resulting matrix C.\n",
        "* blockIdx and threadIdx are used to calculate the row and column indices for the element that each thread will compute.\n",
        "* A loop iterates through the elements of A and B to compute the product and accumulate the sum for the resulting matrix C.\n",
        "\n",
        "**4. Kernel Launch Configuration:**\n",
        "* dim3 structures, threadsPerBlock and blocksPerGrid, are defined to configure the number of threads per block and the number of blocks per grid for kernel execution.\n",
        "\n",
        "**5. Memory Copy Back to Host:**\n",
        "\n",
        "* The resulting matrix C is copied from the device back to the host (CPU) using cudaMemcpy.\n",
        "\n",
        "**6. Printing Matrices:**\n",
        "\n",
        "* The matrices A, B, and the resulting matrix C are printed to the console to display their contents.\n",
        "\n",
        "**7. Memory Cleanup:**\n",
        "\n",
        "* Memory allocated on the GPU is freed using cudaFree.\n",
        "\n",
        "**8. Return:**\n",
        "\n",
        "* The main() function returns 0, indicating successful execution.\n",
        "\n",
        "In summary, this CUDA C code initializes two 4x4 matrices, performs matrix multiplication on the GPU using a kernel function, retrieves the result back to the CPU, and displays the matrices to the console. Adjustments to the matrix values and sizes can be made to suit specific requirements."
      ],
      "metadata": {
        "id": "pfu-KGSGn2Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrixmul.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define SIZE 4\n",
        "\n",
        "__global__ void matrix_multiply(int* A, int* B, int* C) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int sum = 0;\n",
        "    for (int k = 0; k < SIZE; ++k) {\n",
        "        sum += A[row * SIZE + k] * B[k * SIZE + col];\n",
        "    }\n",
        "\n",
        "    C[row * SIZE + col] = sum;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int A[SIZE][SIZE] = {{1, 2, 3, 4},\n",
        "                         {5, 6, 7, 8},\n",
        "                         {9, 10, 11, 12},\n",
        "                         {13, 14, 15, 16}};\n",
        "\n",
        "    int B[SIZE][SIZE] = {{1, 0, 0, 1},\n",
        "                         {0, 1, 1, 0},\n",
        "                         {0, 1, 1, 0},\n",
        "                         {1, 0, 0, 1}};\n",
        "\n",
        "    int C[SIZE][SIZE];\n",
        "\n",
        "    int* d_A, *d_B, *d_C;\n",
        "\n",
        "    cudaMalloc((void**)&d_A, SIZE * SIZE * sizeof(int));\n",
        "    cudaMalloc((void**)&d_B, SIZE * SIZE * sizeof(int));\n",
        "    cudaMalloc((void**)&d_C, SIZE * SIZE * sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_A, A, SIZE * SIZE * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, B, SIZE * SIZE * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 threadsPerBlock(SIZE, SIZE);\n",
        "    dim3 blocksPerGrid(1, 1);\n",
        "\n",
        "    matrix_multiply<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C);\n",
        "\n",
        "    cudaMemcpy(C, d_C, SIZE * SIZE * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Matrix A:\\n\");\n",
        "    for (int i = 0; i < SIZE; ++i) {\n",
        "        for (int j = 0; j < SIZE; ++j) {\n",
        "            printf(\"%d \", A[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"\\nMatrix B:\\n\");\n",
        "    for (int i = 0; i < SIZE; ++i) {\n",
        "        for (int j = 0; j < SIZE; ++j) {\n",
        "            printf(\"%d \", B[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"\\nResultant Matrix C:\\n\");\n",
        "    for (int i = 0; i < SIZE; ++i) {\n",
        "        for (int j = 0; j < SIZE; ++j) {\n",
        "            printf(\"%d \", C[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6feVPTAlmoP",
        "outputId": "18160ba5-b00d-4d67-b3d7-f9162469aed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrixmul.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc matrixmul.cu -o matrixmul"
      ],
      "metadata": {
        "id": "tve2elTKl3Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrixmul"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCVT6EvDmIj1",
        "outputId": "364f2c3a-8b41-4a92-f8d4-fd4dabf08425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            "1 2 3 4 \n",
            "5 6 7 8 \n",
            "9 10 11 12 \n",
            "13 14 15 16 \n",
            "\n",
            "Matrix B:\n",
            "1 0 0 1 \n",
            "0 1 1 0 \n",
            "0 1 1 0 \n",
            "1 0 0 1 \n",
            "\n",
            "Resultant Matrix C:\n",
            "5 5 5 5 \n",
            "13 13 13 13 \n",
            "21 21 21 21 \n",
            "29 29 29 29 \n"
          ]
        }
      ]
    }
  ]
}